---
title: "Literature Review"
output: pdf_document
bibliography: bib/bibliography.bib
---

This literature review discusses the academic study of economic displacement, primarily as it relates to gentrification. We also examine _mass appraisal techniques_, which are automated analytical techniques used for valuing large numbers of real estate properties. Finally, we examine recent applications of machine learning as it relates to predicting gentrification.

## What is Economic Displacement?

Economic displacement has been intertwined with the study of gentrification since shortly after the latter became academically relevant in the 1960s.  The term gentrification was first used in 1964 to describe the _gentry_ in low-income neighborhoods in London [@glass1964]. Originally, academics described gentrification in predominantly positive terms as a "tool of revitalization" for declining neighborhoods [@Zuk2015]. However, by 1979 the negative consequences of gentrification became more well understood, especially with regards to economic exclusion [@Clay1979]. Today, the term has a more neutral connotation, describing the placement and distribution of populations [@Zuk2015]. Specific to cities, recent literature defines gentrification as “the transformation of a working-class or vacant area of the central city into middle-class residential or commercial use” [@Lees2008].

Studies of gentrification and displacement generally take two approaches in the literature: supply-side and demand-side, or "the flows of capital versus flows of people to neighborhoods," respectively [@Zuk2015]. Supply-side arguments for gentrification tend to focus on investments and policies [@Zuk2015] and are much more often the subject of academic literature on economic displacement. This kind of research may be more common because it has the advantage of being more directly linked to influencing public policy. According to @Dreier2004, public policies that can increase economic displacement have been, among others, automobile-oriented transportation infrastructure spending and mortgage interest tax deductions for homeowners. Others who have argued for supply-side gentrification include @Smith1979, who stated that the return of capital from the suburbs to the city, or the "political economy of capital flows into urban areas" are what primarily drive both the positive and negative consequences of urban gentrification.  

More recently, researchers have explored economic displacement as a contributor to income inequality [@Reardon2011; @Watson2009]. Wealthy households tend to influence local political processes to reinforce exclusionary practices. The exercising of political influence by prosperous residents results in a feedback loop producing downward economic pressure on households who lack such resources and influence. Gentrification prediction tools could be used to help break such feedback loops through early identification and intervention. 

Many studies conclude that gentrification in most forms leads to exclusionary economic displacement; however, @Zuk2015 characterizes the results of many recent studies as "mixed, due in part to methodological shortcomings." This work attempts to further the understanding of gentrification prediction by demonstrating a technique to better predict real estate sales in New York City. 

## A Review of Mass Appraisal Techniques

Much research on predicting real estate prices has been in service of creating mass appraisal models. Local governments most commonly use mass appraisal models to assign taxable values to properties. Mass appraisal models share many characteristics with predictive machine learning models in that they are data-driven, standardized methods that employ statistical testing [@Eckert1990]. A variation on mass appraisal models are the _automated valuation models_ (AVM). Both mass appraisal models and AVMs seek to estimate the market value of a single property or several properties through data analysis and statistical modeling [@Springer2017]. 

Scientific mass appraisal models date back to 1936 with the reappraisal of St. Paul, Minnesota [@Silverherz1936]. Since that time, and accelerating with the advent of computers, much statistical research has been done relating property values and rent prices to various characteristics of those properties, including characteristics of their surrounding area. Multiple regression analysis (MRA) has been the most common set of statistical tools used in mass appraisal, including maximum likelihood, weighted least squares, and the most popular, ordinary least squares, or _OLS_ [@Springer2017]. The primary drawbacks of MRA techniques are excessive multicollinearity among attributes and spatial autocorrelation among residuals [@Springer2017]. Another group of models that seek to correct for spatial dependence are known as spatial auto-regressive models (SAR), chief among them the spatial lag model, which aggregates weighted summaries of nearby properties to create independent regression variables [@Springer2017]. 

So-called _hedonic regression models_ seek to decompose the price of a good based on the intrinsic and extrinsic components. @Koschinsky2012 is a recent and thorough discussion of parametric hedonic regression techniques. Koschinsky derives some of the variables included in his models from nearby properties, similar to the techniques used in this work, and these spatial variables were found to be predictive. The basic real estate hedonic model explains the price of a given property as: 

$$
\begin{aligned}
 P_i = P(q_i,S_i, N_i, L_i)
\end{aligned}
$$

\noindent where $P_i$ represents the price of house $i$, which is a "composite good" made up of a vector of structural characteristics $S$, a vector of social and neighborhood characteristics $N$, and a vector of locational characteristics $L$ [@Koschinsky2012 pg. 322]. Specifically, the model calculates spatial lags on properties of interest using neighboring properties within 1,000 feet of a sale. The derived variables include characteristics like average age, the number of poor condition homes, percent of homes with electric heating, construction grades, and more, within 1,000 feet of the property in question. Koschinsky found that in all cases homes near each other were typically similar to each other and priced accordingly. Koschinsky concluded that locational characteristics should be valued at least as much "if not more" than intrinsic structural characteristics.

As recently as 2015, much research has dealt with mitigating the drawbacks of MRA, including the use of multi-level hierarchical models. @Fotheringham2015 explored the combination of geographically weighted regression (GWR) with time-series forecasting to predict home prices over time. GWR is a variation on OLS that allows for adaptive bandwidths of local data to be included, i.e., for each estimate, the number of data points included varies and can be optimized using cross-validation. 


## Predicting Gentrification Using Machine Learning

Both mass appraisal techniques and AVMs seek to predict real estate prices using data and statistical methods; however, traditional techniques typically fall short. These techniques fail partly because property valuation is inherently a “chaotic” process that cannot be modeled effectively using linear methods [@Zuk2015]. The value of any given property is a complex combination of fungible intrinsic characteristics, perceived value, and speculation. The value of any building or plot of land belongs to a rich network where decisions about and perceptions of neighboring properties influence the final market value. @Guan2014 compared traditional MRA techniques to alternative data mining techniques resulting in mixed results. However, as @Helbich2013 states, hedonic pricing models might be improved upon in two primary ways: through novel estimation techniques, and by ancillary structural, locational, and neighborhood variables. Recent research generally falls into these two buckets: better algorithms and better data. 

In the "better data" category, researchers have been striving to introduce new independent variables to increase the accuracy of predictive models. @Dietzell2014 successfully used internet search query data provided by Google Trends to serve as a sentiment indicator and improve commercial real estate forecasting models. @Pivo2011 examined the effects of walkability on property values and investment returns.  Pivo found that on a 100-point scale, a 10-point increase in walkability increased property investment values by up to 9%.
 
Research into better prediction algorithms does not necessarily happen at the exclusion of better data. For example, @Fu2014 created a prediction algorithm, called _ClusRanking_, for real estate in Beijing, China. ClusRanking first estimates neighborhood characteristics using taxi cab traffic vector data, including relative access to business areas. Then, the algorithm performs a rank-ordered prediction of investment returns segmented into five categories. Similar to @Koschinsky2012, though less formally stated, @Fu2014 thought of a property's value as a composite of individual, peer and zone characteristics. In their predictive modeling, Fu included characteristics of the neighborhood (individual), the values of nearby properties (peer), and the prosperity of the affiliated latent business area (zone) based on taxi cab data [@Fu2014].

Several other recent studies compare various advanced statistical techniques and algorithms either to other advanced techniques or to traditional ones. Most studies conclude that the advanced, non-parametric techniques outperform traditional parametric techniques, while several conclude that the Random Forest algorithm is particularly well-suited to predicting real estate values. 

@Kontrimasa2011 compares the accuracy of linear regression against the SVM technique and found the latter to outperform. @Schernthanner2016 compared traditional linear regression techniques to several techniques such as kriging (stochastic interpolation) and Random Forest. They concluded that the more advanced techniques, particularly Random Forest, are sound and more accurate when compared to traditional statistical methods. @antipov12 came to a similar conclusion about the superiority of Random Forest for real estate valuation after comparing 10 algorithms: multiple regression, CHAID, exhaustive CHAID, CART, 2 types of k-nearest neighbors, multilayer perceptron artificial neural network, radial basis function neural network, boosted trees and finally Random Forest. 

@Guan2014 compared three different approaches to defining spatial neighbors: a simple radius technique, a k-nearest neighbors technique using only distance and a k-nearest neighbors technique using all attributes. Interestingly, the location-only KNN models performed best, although by a slight margin. @Park2015 developed several housing-price prediction models based on machine learning algorithms including C4.5, RIPPER, naive Bayesian, and AdaBoost, finding that the RIPPER algorithm consistently outperformed the other models. @Rafiei2016 employed a restricted Boltzmann machine (neural network with back propagation) to predict the sale price of residential condos in Tehran, Iran, using a non-mating genetic algorithm for dimensionality reduction with a focus on computational efficiency. The paper concludes that two  primary strategies help in this regard: weighting property sales by temporal proximity (i.e., sales which happened closer in time are more alike), and also using a learner to accelerate the recognition of important features. The paper compares this technique to several other common neural network approaches and finds that while not necessarily the only way to get the best answer, it is an efficient and fast way to get to the best answer.

Finally, we note that many studies, whether exploring advanced techniques, new data, or both, rely on aggregation of data by some arbitrary boundary.  For example, @Turner2001 predicted gentrification in the Washington, D.C. metro area by ranking census tracts in terms of development. @Chapple2009 created a gentrification early warning system by identifying low-income census tracts in central city locations. @Pollack2010 analyzed 42 census block groups near rail stations in 12 metro areas across the United States, studying changes between 1990 and 2000 for neighborhood socioeconomic and housing characteristics. All of these studies, and many more, relied on the aggregation of data at the census-tract or census-block level. In contrast, this paper compares boundary-aggregation techniques (specifically, aggregating by Zip Codes) to spatial lag techniques and finds the spatial lag techniques to generally outperform. 
