---
title: "Literature Review"
output: pdf_document
bibliography: bib/bibliography.bib
---

The literature review for this paper discusses the concept of Economic Displacement as it has been addressed in academia, primarily in relation to the study of gentrification. We also examine "mass appraisal techniques", which are automated analytical techniques used for valuing large numbers of real estate properties. Finally, we examine recent applications of machine learning as it relates to predicting gentrification.

## What is Economic Displacement?

Economic Displacement has been intertwined with the study of gentrification since shortly after the latter became academically relevant in the 1960's.  The term "gentrification" was first used by Ruth Glass in 1964 to describe the “gentry” in low income neighborhoods in London. Gentrification was originally thought of as a "tool of revitalization for declining neighborhoods" [@Zuk2015], however, in 1979 Phillip Clay highlighted that gentrification brought both positive and negative consequences to local populations and described the negative as Economic Exclusion [@Clay1979]. Today, the term has adopted a more neutral connotation, describing the "spatial organization and re-organization of human dwelling and activity" [@Zuk2015]. Specific to cities, gentrification is thought of as “the transformation of a working-class or vacant area of the central city into middle-class residential or commercial use” [@Lees2008].

Studies of gentrification and displacement generally take two approaches in the literature: supply-side and demand-side, or "the flows of capital versus flows of people to neighborhoods", respectively [@Zuk2015]. Supply side arguments for gentrification tend to focus on "private capital investment, public policies, and public investments" [@Zuk2015], and are much more often the subject of academic literature on Economic Displacement. This kind of research may be more common because it has the advantage of being more directly linked to influencing public policy (as opposed to controlling the flows of people). According to @Dreier2004, public policies that can increase Economic Displacement have been, among others, automobile-oriented transportation infrastructure spending and mortgage interest tax deductions for home owners. Others that have argued for supply-side gentrification include @Smith1979, who stated that the return of capital from the suburbs to the city, or the "political economy of capital flows into urban areas" are what primarily drive both the positive and negative consequences of urban gentrification.  

More recently, Economic Displacement has been explored as a contributor to income inequality [@Reardon2011]; [@Watson2009]. Concentrations of wealth allow "certain households to sort themselves according to their preferences – and control local political processes that continue exclusion" [@Reardon2011]. This results in a self-reinforcing feedback loop where wealthier households influence public policy toward their self interest. Gentrification prediction tools could be used to help break such feedback loops through early identification and intervention. 

Many studies conclude that gentrification in most forms leads to exclusionary economic displacement, however, @Zuk2015 characterizes the results of many recent studies as "mixed, due in part to methodological shortcomings". In this paper, we attempt to further the understanding of gentrification prediction by demonstrating a technique to better predict real estate sales in New York City. 

## A Review of Mass Appraisal Techniques

Much of the research on predicting real estate prices has been in service of creating mass appraisal models. Mass appraisal models are most commonly used by local governments for the purpose of collecting taxes from property owners. Mass appraisal models share many characteristics with predictive machine learning models, in that they are data-driven, standardized methods that employ statistical testing [@Eckert1990]. A variation on mass appraisal models are the "automated valuation models" (AVM), which use "often the same methodological framework of mass appraisal... a statistical model and a large amount of property data to estimate the market value of an individual property or portfolio of properties" [@Springer2017]. 

Scientific mass appraisal models date back to 1936 with the reappraisal of St. Paul, Minnesota [@Silverherz1936]. Since that time, and accelerated with the advent of computers, much statistical research has been done relating property values and rent prices to various characteristics of those properties, including characteristics of their surrounding area. Multiple regression analysis (MRA) has been the most common set of statistical tools used in mass appraisal, including Maximum Likelihood, Weighted Least Squares, and the most popular, Ordinary Least Squares (OLS) [@Springer2017]. The primary drawbacks of MRA techniques are "excessive multicollinearity among attributes" and "spatial autocorrelation among residuals" [@Springer2017]. Another group of models that seek to correct for spatial dependence are known as Spatial Auto Regressive models (SAR), chief among them the Spatial Lag Model, which aggregates weighted summaries of nearby properties in order to create independent regression variables [@Springer2017]. 

So-called "Hedonic" regression models seek to decompose the price of a good based on the intrinsic and extrinsic components. @Koschinsky2012 is a recent and thorough discussion of parametric hedonic regression techniques. Some of the variables included in Koschinsky's models are derived from nearby properties, similar to the technique used in this paper, and these variables were found to be predictive. The real estate hedonic model as defined by Koschinsky describes the price of a property as: 

$$
\begin{aligned}
 P_i = P(S_i, N_i, L_i)
\end{aligned}
$$

Where $P_i$ represents the price of house $i$, which is a composite good comprised of a vector of structural characteristics $S$, a vector of social and neighborhood characteristics $N$, and a vector of locational characteristics $L$. Specifically, the model calculates spatial lags on properties of interest using neighboring properties within 1,000 feet of a sale. The derived variables include characteristics like average age, quantity of poor condition homes, percent of homes with electric heating, construction grade, etc., within 1,000 feet of the property in question. Koschinsky found that in all cases, "the relation between a home’s price and the average price of its neighboring homes is characterized by positive spatial autocorrelation" meaning that homes near each other were typically similar to each other and priced accordingly. Koschinsky concluded that locational characteristics should be valued at least as much "if not more" than important structural characteristics.

As recently as 2015, much research has dealt with mitigating the drawbacks of MRA, including the use of multi-level hierarchical models. @Fotheringham2015 explored the combination of Geographically Weighted Regression (GWR) with time-series forecasting to predict home prices over time. GWR is a variation on OLS that allows for "adaptive bandwidths" of local data to be included, i.e., for each estimate, the number of data points included varies and can be optimized using cross-validation. 


## Prediction of Gentrification Using Machine Learning

Both Mass Appraisal techniques and Automated Valuation Modeling  seek to predict real estate prices using data and statistical methods, however, traditional techniques typically fall short. This is because property valuation is inherently a “chaotic” process that "does not lend itself to binary or linear analysis" [@Zuk2015]. The value of any given property is a complex combination of fungible intrinsic characteristics, perceived value and speculation. The value of any building or plot of land belongs to a rich network where decisions about and perceptions of neighboring properties influence the final market value. @Guan2014 compared traditional MRA techniques to alternative "data mining techniques" resulting in "mixed results". However, as @Helbich2013 states, hedonic pricing models "can be improved in two ways: (a) Through novel estimation techniques, and (b) by ancillary structural, locational, and neighborhood variables on the basis of Geographic Information System (GIS)". Recent research generally falls into these two buckets: better analysis algorithms and/or better data. 

In the "better data" category, researchers have been striving to introduce new independent variables to increase the accuracy of predictive models. @Dietzell2014 successfully used internet search query data provided by Google Trends to serve as a sentiment indicator and improve commercial real estate forecasting models. @Pivo2011 examined the effects of walkability on property values and investment returns.  Pivo found that on a 100-point scale, a 10-point increase in walkability increased property investment values by up to 9%.
 
Research into better prediction algorithms do not necessarily happen at the exclusion of "better data". For example, @Fu2014 created a prediction algorithm, called "ClusRanking", for real estate in Beijing, China. ClusRanking first estimates neighborhood characteristics using taxi cab traffic vector data, specifically as they relate to accessibility to "business areas". Then, the algorithm performs a rank-ordered prediction of investment returns segmented into five categories. Similar to @Koschinsky2012, though less formally stated, @Fu2014 thought of a property's value as a composite of individual, peer and zone characteristics. In the predictive model, Fu includes characteristics of the neighborhood (individual), the values of its nearby properties (peer), and the prosperity of the affiliated latent business area (zone) based on taxi cab data [@Fu2014].

Several other recent studies compare various "advanced" statistical techniques and algorithms either to other advanced techniques or to traditional ones. Most studies conclude that the advanced, non-parametric techniques outperform traditional parametric techniques, while several conclude that the Random Forest algorithm is particularly well-suited to predicting real estate values. 

@Kontrimasa2011 compares the accuracy of linear regression against the SVM technique and found the latter to outperform. @Schernthanner2016 compared traditional linear regression techniques to several techniques such as krigging (stochastic interpolation) and Random Forest. They concluded that the more advanced techniques, particularly Random Forest, are sound and more accurate when compared to traditional statistical methods. @antipov12 came to a similar conclusion about the superiority of Random Forest for real estate valuation after comparing 10 algorithms: multiple regression, CHAID, Exhaustive CHAID, CART, 2 types of k-Nearest Neighbors, Multilayer Perceptron artificial neural network (ANN), Radial Basis Function neural network (RBF), Boosted Trees and finally Random Forest. 

@Guan2014 compared three different approaches to defining spatial neighbors: a simple radius technique, a k-nearest neighbors technique using only distance and a k-nearest neighbors technique using all attributes. Interestingly, the location-only KNN models performed best, although by a slight margin. @Park2015 developed several housing price prediction models based on machine learning algorithms including C4.5, RIPPER, Naive Bayesian, and AdaBoost, finding that the RIPPER algorithm consistently outperformed the other models in the performance of housing price prediction. @Rafiei2016 employed a restricted boltzmann machine (neural network with back propagation) to predict the sale price of residential condos in Tehran, Iran, using a non-mating genetic algorithm for dimensionality reduction with a focus on computational efficiency. The paper concludes that two  primary strategies help in this regard: weighting property sales by temporal proximity (i.e., sales which happened closer in time are more alike), and also using a learner to accelerate the recognition of important features. The paper compares this technique to several other common neural network approaches and finds that while not necessarily the only way to get the best answer, it is an efficient and fast way to get to the best answer.

Finally, it should be noted that many studies, whether exploring advanced techniques, new data, or both, rely on aggregation of data by some arbitrary boundary.  For example, @Turner2001 predicted gentrification in the Washington, D.C. metro area by ranking census tracts in terms of development. @Chapple2009 created a gentrification "early warning system" by identifying low income census tracts in central city locations. @Pollack2010 analyzed 42 census block groups near rail stations in 12 metro areas across the United States, studying changes between 1990 and 2000 for neighborhood socioeconomic and housing characteristics. All of these studies, and many more, relied on aggregation of data at the census-tract or census-block level. In contrast, this paper compares boundary-aggregation techniques (specifically, aggregating by zip codes) to spatial-lag techniques and finds the spatial lag techniques to generally outperform. 
